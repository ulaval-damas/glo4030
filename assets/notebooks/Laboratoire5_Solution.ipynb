{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratoire 5: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deeplib.training import train, test\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "from deeplib.datasets import load_cifar10, load_mnist\n",
    "from deeplib.visualization import view_filters\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from random import randrange\n",
    "\n",
    "cifar_train, cifar_test = load_cifar10()\n",
    "mnist_train, mnist_test = load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Filtres de convolution\n",
    "\n",
    "Lors de l'entraînement, le réseau apprend les bons paramètres à utiliser. Par contre, autrefois, il fallait utiliser des filtres fait à la main comme [les filtres de Gabor](https://en.wikipedia.org/wiki/Gabor_filter).\n",
    "\n",
    "### Exercice\n",
    "\n",
    "Le réseau suivant contient une seule couche de convolution. \n",
    "\n",
    "Créez manuellement quelques fitres que vous utiliserez pour faire de la classification sur CIFAR10. \n",
    "\n",
    "Par la suite, figez les poids de la couche de convolution et entraînez le réseau. \n",
    "Tentez d'obtenir les meilleurs résultat possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3, padding=1)\n",
    "        self.fc = nn.Linear(6 * 14 * 14, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = x.view(-1, 6*14*14)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifier les paramètres des filtres. Essayez de faire des filtres permettant d'extraire des caractéristiques de bas niveau (ligne, coin, etc...). Vous pouvez consulter [ceci](http://lodev.org/cgtutor/filtering.html) pour avoir des idées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filters = []\n",
    "filters.append([[[0, 0, 0],\n",
    "                 [0, 1, 0],\n",
    "                 [0, 0, 0]]]) # Ce filtre retourne l'image original\n",
    "\n",
    "# Arete verticales\n",
    "filters.append([[[-1, 0, 1],\n",
    "                 [-1, 0, 1],\n",
    "                 [-1, 0, 1]]])\n",
    "\n",
    "# Aretes horizontales\n",
    "filters.append([[[-1, -1, -1],\n",
    "                 [0, 0, 0],\n",
    "                 [1, 1, 1]]])\n",
    "\n",
    "# Motion blur (gaussien)\n",
    "filters.append([[[1, 2, 1],\n",
    "                 [2, 4, 2],\n",
    "                 [1, 2, 1]]])\n",
    "\n",
    "# Arete toute directions\n",
    "filters.append([[[-1, -1, -1],\n",
    "                 [-1, 8, -1],\n",
    "                 [-1, -1, -1]]])\n",
    "\n",
    "# Emboss (shadow effect of image)\n",
    "filters.append([[[-1, -1, 0],\n",
    "                 [-1, 0, 1],\n",
    "                 [0, 1, 1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# On crée le réseau, remplace les paramètres par les filtres précédents et fige les poids.\n",
    "\n",
    "net = Net()\n",
    "filters = np.asarray(filters, dtype=np.float32)\n",
    "net.conv1.weight.data = torch.from_numpy(filters)\n",
    "for param in net.conv1.parameters():\n",
    "    param.requires_grad = False\n",
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vous pouvez utiliser cette cellule pour visualiser l'effet de vos filtres sur des images du dataset.\n",
    "for i in range(3):\n",
    "    image, label = mnist_train[randrange(0, len(mnist_train))]\n",
    "    view_filters(net, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "n_epoch = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.fc.parameters(), lr=lr) # On optimise uniquement la couche pleinement connectée.\n",
    "history = train(net, optimizer, mnist_train, n_epoch, batch_size)\n",
    "history.display_accuracy()\n",
    "history.display_loss()\n",
    "print('Précision en test: {:.2f}'.format(test(net, mnist_test, batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture de base\n",
    "\n",
    "### Exercice\n",
    "\n",
    "Implémentez une architecture de base de réseau de neurones à convolution ayant les caractéristiques suivantes.\n",
    "\n",
    "1. 3 couches de convolution\n",
    "2. Toutes les couches ont 100 filtres de tailles 3x3 et 1px de padding.\n",
    "3. Batch normalization après chaque couche.\n",
    "4. Maxpooling avec un noyau de taille 2 après les 2 premières couches.\n",
    "5. 1 seule couche linéaire pour la classification (aucune activation nécessaire)\n",
    "6. Utiliser la ReLu comme fonction d'activation.\n",
    "\n",
    "Notez que la taille des images de CIFAR10 est de 3x32x32 (images en couleur). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 100, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.fc = nn.Linear(100 * 8 * 8, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.bn1(self.conv1(x))), 2)\n",
    "        x = F.max_pool2d(F.relu(self.bn2(self.conv2(x))), 2)\n",
    "        x = F.relu(self.bn2(self.conv3(x)))\n",
    "        x = x.view(-1, 100 * 8 * 8)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "n_epoch = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "history = train(model, optimizer, cifar_train, n_epoch, batch_size)\n",
    "history.display_accuracy()\n",
    "history.display_loss()\n",
    "print('Précision en test: {:.2f}'.format(test(model, cifar_test, batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture profonde\n",
    "\n",
    "En deep learning, on dit que la performance augmente avec le nombre de couches.\n",
    "\n",
    "### Exercice\n",
    "\n",
    "Ajoutez 2 couches de convolution de 100 filtres dans le réseau précédent (n'oubliez pas la batch normalization et le padding). Mettez du maxpooling après la couche 1 et 3. Comparez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 100, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.fc = nn.Linear(100 * 8 * 8, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.bn1(self.conv1(x))), 2)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(F.relu(self.bn3(self.conv3(x))), 2)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = x.view(-1, 100 * 8 * 8)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "history = train(model, optimizer, cifar_train, n_epoch, batch_size)\n",
    "history.display_accuracy()\n",
    "history.display_loss()\n",
    "print('Précision en test: {:.2f}'.format(test(model, cifar_test, batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice\n",
    "\n",
    "Ajoutez 4 autres couches de 100 filtres avec batchnorm et padding de 1. Mettez le maxpooling après les couches 3 et 5.\n",
    "\n",
    "Est-ce que les résultats continuent à s'améliorer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 100, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.conv8 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.conv9 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn9 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.fc = nn.Linear(100 * 8 * 8, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(F.relu(self.bn3(self.conv3(x))), 2)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.max_pool2d(F.relu(self.bn5(self.conv5(x))), 2)\n",
    "        x = F.relu(self.bn6(self.conv6(x)))\n",
    "        x = F.relu(self.bn7(self.conv7(x)))\n",
    "        x = F.relu(self.bn8(self.conv8(x)))\n",
    "        x = F.relu(self.bn9(self.conv9(x)))\n",
    "        x = x.view(-1, 100 * 8 * 8)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "model.cuda()\n",
    "\n",
    "lr = 0.0005\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "history = train(model, optimizer, cifar_train, n_epoch, batch_size)\n",
    "history.display_accuracy()\n",
    "history.display_loss()\n",
    "print('Précision en test: {:.2f}'.format(test(model, cifar_test, batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion résiduelle\n",
    "\n",
    "Ajouter de plus en plus de couche augmente aussi la difficulté avec laquelle le gradient peut se propager dans le réseau. Une des solutions suggérés est d'utiliser une connexion résiduelle permettant au gradient de _sauter_ des couches. Dans l'article présentant cette connexion, elle est définie comme suit:\n",
    "\n",
    "![alt text](http://cv-tricks.com/wp-content/uploads/2017/03/600x299xResNet.png.pagespeed.ic.ZVwbFN7vyG.webp \"Connexion résiduelle\")\n",
    "\n",
    "### Exercice\n",
    "Reprenez l'architecture précédente et ajouter des connexion résiduelle à chaque 2 couches en commençant à la couche 2. Faites un maxpool après la connexion résiduelle suivant la couche 3 et 5.\n",
    "\n",
    "Comparez les résultats et la vitesse avec laquelle le réseau a entraîné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Resnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Resnet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 100, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(100)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(100)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(100)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(100)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(100)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(100)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(100)\n",
    "\n",
    "        self.conv8 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(100)\n",
    "\n",
    "        self.conv9 = nn.Conv2d(100, 100, 3, padding=1)\n",
    "        self.bn9 = nn.BatchNorm2d(100)\n",
    "\n",
    "        self.fc1 = nn.Linear(100 * 8 * 8, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        res = x\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        x = F.relu(x + res)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        res = x\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.bn5(self.conv5(x))\n",
    "        x = F.relu(x + res)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        res = x\n",
    "        x = F.relu(self.bn6(self.conv6(x)))\n",
    "        x = self.bn7(self.conv7(x))\n",
    "        x = F.relu(x + res)\n",
    "\n",
    "        res = x\n",
    "        x = F.relu(self.bn8(self.conv8(x)))\n",
    "        x = self.bn9(self.conv9(x))\n",
    "        x = F.relu(x + res)\n",
    "\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def num_flat_features(x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Resnet()\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "history = train(model, optimizer, cifar_train, n_epoch, batch_size)\n",
    "history.display_accuracy()\n",
    "history.display_loss()\n",
    "print('Précision en test: {:.2f}'.format(test(model, cifar_test, batch_size)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
